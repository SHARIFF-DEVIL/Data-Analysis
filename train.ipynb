{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de6efb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tables: [('Power',), ('users',)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('Dataset.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute a query to get the list of tables\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "\n",
    "# Fetch all results\n",
    "tables = cur.fetchall()\n",
    "\n",
    "# Print the names of the tables\n",
    "print(\"Available tables:\", tables)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c42cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Dates   PX_OPEN   PX_HIGH    PX_LOW   PX_LAST PX_VOLUME  \\\n",
      "0  2000-01-03 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "1  2000-01-04 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "2  2000-01-05 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "3  2000-01-06 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "4  2000-01-07 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "\n",
      "  MOV_AVG_5D MOV_AVG_10D MOV_AVG_30D MOV_AVG_50D MOV_AVG_100D MOV_AVG_200D  \\\n",
      "0   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "1   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "2   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "3   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "4   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "\n",
      "     RSI_3D    RSI_9D   RSI_14D   RSI_30D NEWS_SENTIMENT_DAILY_AVG  \\\n",
      "0  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "1  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "2  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "3  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "4  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "\n",
      "  TWITTER_SENTIMENT_DAILY_AVG  \n",
      "0                    #N/A N/A  \n",
      "1                    #N/A N/A  \n",
      "2                    #N/A N/A  \n",
      "3                    #N/A N/A  \n",
      "4                    #N/A N/A  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Convert Date Column to Datetime\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, dayfirst\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Set Date as Index\u001b[39;00m\n\u001b[0;32m     18\u001b[0m df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Date'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('Dataset.db')\n",
    "\n",
    "# Load data from the SQLite database\n",
    "query = \"SELECT * FROM Power\"  # Replace 'stocks' with your actual table name\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Inspect the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Convert Date Column to Datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "\n",
    "# Set Date as Index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Handle Missing Values\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Resample Data (e.g., to weekly frequency)\n",
    "df_resampled = df.resample('W').mean()  # Adjust the frequency as needed\n",
    "\n",
    "# Close Database Connection\n",
    "conn.close()\n",
    "\n",
    "# Display the resampled DataFrame\n",
    "print(df_resampled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "660a19c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: Index(['Dates', 'PX_OPEN', 'PX_HIGH', 'PX_LOW', 'PX_LAST', 'PX_VOLUME',\n",
      "       'MOV_AVG_5D', 'MOV_AVG_10D', 'MOV_AVG_30D', 'MOV_AVG_50D',\n",
      "       'MOV_AVG_100D', 'MOV_AVG_200D', 'RSI_3D', 'RSI_9D', 'RSI_14D',\n",
      "       'RSI_30D', 'NEWS_SENTIMENT_DAILY_AVG', 'TWITTER_SENTIMENT_DAILY_AVG'],\n",
      "      dtype='object')\n",
      "                 Dates   PX_OPEN   PX_HIGH    PX_LOW   PX_LAST PX_VOLUME  \\\n",
      "0  2000-01-03 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "1  2000-01-04 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "2  2000-01-05 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "3  2000-01-06 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "4  2000-01-07 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "\n",
      "  MOV_AVG_5D MOV_AVG_10D MOV_AVG_30D MOV_AVG_50D MOV_AVG_100D MOV_AVG_200D  \\\n",
      "0   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "1   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "2   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "3   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "4   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "\n",
      "     RSI_3D    RSI_9D   RSI_14D   RSI_30D NEWS_SENTIMENT_DAILY_AVG  \\\n",
      "0  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "1  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "2  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "3  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "4  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "\n",
      "  TWITTER_SENTIMENT_DAILY_AVG  \n",
      "0                    #N/A N/A  \n",
      "1                    #N/A N/A  \n",
      "2                    #N/A N/A  \n",
      "3                    #N/A N/A  \n",
      "4                    #N/A N/A  \n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [2000-01-09 00:00:00, 2000-01-16 00:00:00, 2000-01-23 00:00:00, 2000-01-30 00:00:00, 2000-02-06 00:00:00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nupur\\AppData\\Local\\Temp\\ipykernel_12432\\829341602.py:36: FutureWarning: Dropping invalid columns in DataFrameGroupBy.mean is deprecated. In a future version, a TypeError will be raised. Before calling .mean, select only columns which should be valid for the function.\n",
      "  df_resampled = df.resample('W').mean()  # Adjust the frequency as needed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('Dataset.db')\n",
    "\n",
    "# Load data from the SQLite database\n",
    "query = \"SELECT * FROM Power\"  # Replace 'stocks' with your actual table name\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Inspect the DataFrame\n",
    "print(\"Columns in the DataFrame:\", df.columns)  # Check the actual column names\n",
    "print(df.head())  # Display the first few rows\n",
    "\n",
    "# Check if 'Date' column exists and adjust accordingly\n",
    "if 'Date' in df.columns:\n",
    "    # Convert Date Column to Datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "else:\n",
    "    # If 'Date' is not found, check for variations\n",
    "    date_column = [col for col in df.columns if 'date' in col.lower()]\n",
    "    if date_column:\n",
    "        df[date_column[0]] = pd.to_datetime(df[date_column[0]], dayfirst=True)\n",
    "        df.set_index(date_column[0], inplace=True)\n",
    "    else:\n",
    "        raise KeyError(\"No date column found in the DataFrame.\")\n",
    "\n",
    "# Set Date as Index if not already set\n",
    "if 'Date' in df.columns:\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "# Handle Missing Values\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Resample Data (e.g., to weekly frequency)\n",
    "df_resampled = df.resample('W').mean()  # Adjust the frequency as needed\n",
    "\n",
    "# Close Database Connection\n",
    "conn.close()\n",
    "\n",
    "# Display the resampled DataFrame\n",
    "print(df_resampled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71a8a9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: Index(['Dates', 'PX_OPEN', 'PX_HIGH', 'PX_LOW', 'PX_LAST', 'PX_VOLUME',\n",
      "       'MOV_AVG_5D', 'MOV_AVG_10D', 'MOV_AVG_30D', 'MOV_AVG_50D',\n",
      "       'MOV_AVG_100D', 'MOV_AVG_200D', 'RSI_3D', 'RSI_9D', 'RSI_14D',\n",
      "       'RSI_30D', 'NEWS_SENTIMENT_DAILY_AVG', 'TWITTER_SENTIMENT_DAILY_AVG'],\n",
      "      dtype='object')\n",
      "                 Dates   PX_OPEN   PX_HIGH    PX_LOW   PX_LAST PX_VOLUME  \\\n",
      "0  2000-01-03 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "1  2000-01-04 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "2  2000-01-05 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "3  2000-01-06 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "4  2000-01-07 00:00:00  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A   \n",
      "\n",
      "  MOV_AVG_5D MOV_AVG_10D MOV_AVG_30D MOV_AVG_50D MOV_AVG_100D MOV_AVG_200D  \\\n",
      "0   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "1   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "2   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "3   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "4   #N/A N/A    #N/A N/A    #N/A N/A    #N/A N/A     #N/A N/A     #N/A N/A   \n",
      "\n",
      "     RSI_3D    RSI_9D   RSI_14D   RSI_30D NEWS_SENTIMENT_DAILY_AVG  \\\n",
      "0  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "1  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "2  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "3  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "4  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A                 #N/A N/A   \n",
      "\n",
      "  TWITTER_SENTIMENT_DAILY_AVG  \n",
      "0                    #N/A N/A  \n",
      "1                    #N/A N/A  \n",
      "2                    #N/A N/A  \n",
      "3                    #N/A N/A  \n",
      "4                    #N/A N/A  \n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [2000-01-09 00:00:00, 2000-01-16 00:00:00, 2000-01-23 00:00:00, 2000-01-30 00:00:00, 2000-02-06 00:00:00]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('Dataset.db')\n",
    "\n",
    "# Load data from the SQLite database\n",
    "query = \"SELECT * FROM Power\"  \n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Inspect the DataFrame\n",
    "print(\"Columns in the DataFrame:\", df.columns)  # Check the actual column names\n",
    "print(df.head())  # Display the first few rows\n",
    "\n",
    "# Check if 'Date' column exists and adjust accordingly\n",
    "if 'Date' in df.columns:\n",
    "    # Convert Date Column to Datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "else:\n",
    "    # If 'Date' is not found, check for variations\n",
    "    date_column = [col for col in df.columns if 'date' in col.lower()]\n",
    "    if date_column:\n",
    "        df[date_column[0]] = pd.to_datetime(df[date_column[0]], dayfirst=True)\n",
    "        df.set_index(date_column[0], inplace=True)\n",
    "    else:\n",
    "        raise KeyError(\"No date column found in the DataFrame.\")\n",
    "\n",
    "# Set Date as Index if not already set\n",
    "if 'Date' in df.columns:\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "# Handle Missing Values\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Select only numeric columns for resampling\n",
    "numeric_df = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Resample Data (e.g., to weekly frequency) and calculate the mean\n",
    "df_resampled = numeric_df.resample('W').mean()  # Adjust the frequency as needed\n",
    "\n",
    "# Close Database Connection\n",
    "conn.close()\n",
    "\n",
    "# Display the resampled DataFrame\n",
    "print(df_resampled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca6b5a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: Index(['Dates', 'PX_OPEN', 'PX_HIGH', 'PX_LOW', 'PX_LAST', 'PX_VOLUME',\n",
      "       'MOV_AVG_5D', 'MOV_AVG_10D', 'MOV_AVG_30D', 'MOV_AVG_50D',\n",
      "       'MOV_AVG_100D', 'MOV_AVG_200D', 'RSI_3D', 'RSI_9D', 'RSI_14D',\n",
      "       'RSI_30D', 'NEWS_SENTIMENT_DAILY_AVG', 'TWITTER_SENTIMENT_DAILY_AVG'],\n",
      "      dtype='object')\n",
      "Data types in the DataFrame:\n",
      " Dates                          object\n",
      "PX_OPEN                        object\n",
      "PX_HIGH                        object\n",
      "PX_LOW                         object\n",
      "PX_LAST                        object\n",
      "PX_VOLUME                      object\n",
      "MOV_AVG_5D                     object\n",
      "MOV_AVG_10D                    object\n",
      "MOV_AVG_30D                    object\n",
      "MOV_AVG_50D                    object\n",
      "MOV_AVG_100D                   object\n",
      "MOV_AVG_200D                   object\n",
      "RSI_3D                         object\n",
      "RSI_9D                         object\n",
      "RSI_14D                        object\n",
      "RSI_30D                        object\n",
      "NEWS_SENTIMENT_DAILY_AVG       object\n",
      "TWITTER_SENTIMENT_DAILY_AVG    object\n",
      "dtype: object\n",
      "Checking for NaN values in the DataFrame:\n",
      "PX_OPEN                        2512\n",
      "PX_HIGH                        2512\n",
      "PX_LOW                         2512\n",
      "PX_LAST                        2512\n",
      "PX_VOLUME                      2513\n",
      "MOV_AVG_5D                     2516\n",
      "MOV_AVG_10D                    2521\n",
      "MOV_AVG_30D                    2543\n",
      "MOV_AVG_50D                    2566\n",
      "MOV_AVG_100D                   2619\n",
      "MOV_AVG_200D                   2724\n",
      "RSI_3D                         2515\n",
      "RSI_9D                         2521\n",
      "RSI_14D                        2526\n",
      "RSI_30D                        2545\n",
      "NEWS_SENTIMENT_DAILY_AVG       2497\n",
      "TWITTER_SENTIMENT_DAILY_AVG    3918\n",
      "dtype: int64\n",
      "Mean Squared Error: 0.25513719819718667\n",
      "Monthly Average Data:\n",
      "              PX_OPEN    PX_HIGH     PX_LOW    PX_LAST     PX_VOLUME  \\\n",
      "Dates                                                                  \n",
      "2015-01-31  47.344118  48.517647  46.529412  47.711765  8.473447e+06   \n",
      "2015-02-28  51.742500  52.985000  50.635000  51.810000  8.642160e+06   \n",
      "2015-03-31  51.631818  52.386364  50.127273  51.018182  9.930094e+06   \n",
      "2015-04-30  46.795455  47.415909  45.847727  46.579545  4.248574e+06   \n",
      "2015-05-31  41.623810  42.247619  40.811905  41.361905  4.498284e+06   \n",
      "\n",
      "            MOV_AVG_5D  MOV_AVG_10D  MOV_AVG_30D  MOV_AVG_50D  MOV_AVG_100D  \\\n",
      "Dates                                                                         \n",
      "2015-01-31   46.710000    45.970294    44.213724    44.836941     46.154029   \n",
      "2015-02-28   51.274500    50.432500    47.934830    46.130350     45.974900   \n",
      "2015-03-31   51.740909    52.780455    52.437273    50.179045     47.520341   \n",
      "2015-04-30   46.882273    47.170682    49.761823    50.698545     47.919659   \n",
      "2015-05-31   41.697619    42.162143    44.769138    47.273143     47.721190   \n",
      "\n",
      "            MOV_AVG_200D     RSI_3D     RSI_9D    RSI_14D    RSI_30D  \\\n",
      "Dates                                                                  \n",
      "2015-01-31     51.237724  70.177324  65.802171  61.744406  54.413594   \n",
      "2015-02-28     51.243970  57.111045  60.581005  60.511510  57.409265   \n",
      "2015-03-31     51.245632  34.725091  42.680291  46.894518  51.113555   \n",
      "2015-04-30     49.991695  43.690027  38.998923  40.487418  45.452114   \n",
      "2015-05-31     48.267967  37.427838  37.192800  37.515795  41.116986   \n",
      "\n",
      "            NEWS_SENTIMENT_DAILY_AVG  TWITTER_SENTIMENT_DAILY_AVG  \n",
      "Dates                                                              \n",
      "2015-01-31                  0.293488                     0.018829  \n",
      "2015-02-28                  0.269200                    -0.039035  \n",
      "2015-03-31                  0.108918                    -0.051873  \n",
      "2015-04-30                  0.134236                    -0.016841  \n",
      "2015-05-31                  0.018181                    -0.004400  \n",
      "Predictions vs Actual:\n",
      "               Actual  Predicted\n",
      "Dates                           \n",
      "2018-09-30  29.265000  29.092223\n",
      "2018-12-31  52.192857  52.041542\n",
      "2015-05-31  41.361905  41.342294\n",
      "2019-08-31  59.211364  59.008610\n",
      "2017-03-31  39.132609  39.290405\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 1: Connect to the SQLite database\n",
    "conn = sqlite3.connect('Dataset.db')\n",
    "\n",
    "# Step 2: Load data from the SQLite database\n",
    "query = \"SELECT * FROM Power\"  # Replace 'Power' with your actual table name\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Step 3: Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Step 4: Check the column names and data types in the DataFrame\n",
    "print(\"Columns in the DataFrame:\", df.columns)\n",
    "print(\"Data types in the DataFrame:\\n\", df.dtypes)\n",
    "\n",
    "# Step 5: Clean the DataFrame\n",
    "# Replace '#N/A N/A' with NaN\n",
    "df.replace('#N/A N/A', pd.NA, inplace=True)\n",
    "\n",
    "# Step 6: Convert the 'Dates' column to datetime and set it as the index\n",
    "if 'Dates' in df.columns:\n",
    "    df['Dates'] = pd.to_datetime(df['Dates'], dayfirst=True)  # Adjust if necessary\n",
    "    df.set_index('Dates', inplace=True)\n",
    "else:\n",
    "    print(\"Error: 'Dates' column not found in the DataFrame.\")\n",
    "    raise KeyError(\"No date column found in the DataFrame.\")\n",
    "\n",
    "# Step 7: Convert relevant columns to numeric\n",
    "numeric_columns = ['PX_OPEN', 'PX_HIGH', 'PX_LOW', 'PX_LAST', 'PX_VOLUME', \n",
    "                   'MOV_AVG_5D', 'MOV_AVG_10D', 'MOV_AVG_30D', \n",
    "                   'MOV_AVG_50D', 'MOV_AVG_100D', 'MOV_AVG_200D', \n",
    "                   'RSI_3D', 'RSI_9D', 'RSI_14D', 'RSI_30D', \n",
    "                   'NEWS_SENTIMENT_DAILY_AVG', 'TWITTER_SENTIMENT_DAILY_AVG']\n",
    "\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to numeric, set non-numeric to NaN\n",
    "\n",
    "# Step 8: Check for NaN values in the DataFrame\n",
    "print(\"Checking for NaN values in the DataFrame:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Step 9: Handle NaN values\n",
    "# Option 1: Drop rows with NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Option 2: Alternatively, you can fill NaN values (uncomment the line below to use it)\n",
    "# df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "\n",
    "# Step 10: Select only numeric columns for resampling\n",
    "numeric_df = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Step 11: Resample the data to monthly frequency and calculate the mean\n",
    "monthly_average_df = numeric_df.resample('M').mean()\n",
    "\n",
    "# Step 12: Prepare the data for training\n",
    "# Check if 'PX_LAST' is in the monthly average DataFrame\n",
    "if 'PX_LAST' in monthly_average_df.columns:\n",
    "    X = monthly_average_df.drop(columns=['PX_LAST'])  # Use all columns except 'PX_LAST' as features\n",
    "    y = monthly_average_df['PX_LAST']\n",
    "else:\n",
    "    print(\"Error: 'PX_LAST' column not found in the monthly average DataFrame.\")\n",
    "    X = None\n",
    "    y = None\n",
    "\n",
    "# Step 13: Proceed only if X and y are defined\n",
    "if X is not None and y is not None:\n",
    "    # Step 14: Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Step 15: Train a Linear Regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Step 16: Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Step 17: Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "    # Step 18: Create an output directory to save results\n",
    "    output_dir = 'output'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Step 19: Save the monthly average DataFrame and predictions to CSV files\n",
    "    monthly_average_df.to_csv(os.path.join(output_dir, 'monthly_average_data.csv'))\n",
    "    results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "    results_df.to_csv(os.path.join(output_dir, 'predictions.csv'))\n",
    "\n",
    "    # Step 20: Display the first few rows of the results\n",
    "    print(\"Monthly Average Data:\")\n",
    "    print(monthly_average_df.head())\n",
    "    print(\"Predictions vs Actual:\")\n",
    "    print(results_df.head())\n",
    "else:\n",
    "    print(\"Training could not be performed due to missing target variable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "625714ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PX_OPEN   PX_LAST   PX_HIGH    PX_LOW PX_VOLUME\n",
      "0  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A\n",
      "1  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A\n",
      "2  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A\n",
      "3  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A\n",
      "4  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A  #N/A N/A\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Step 1: Connect to the SQLite database\n",
    "conn = sqlite3.connect('Dataset.db')\n",
    "\n",
    "# Step 2: Load data from the SQLite database\n",
    "query = \"SELECT * FROM Power\"  # Replace 'Power' with your actual table name\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Step 3: Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Step 4: Remove rows containing NA values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 5: Keep only the specified columns\n",
    "columns_to_keep = ['PX_OPEN', 'PX_LAST', 'PX_HIGH', 'PX_LOW', 'PX_VOLUME']\n",
    "df_filtered = df[columns_to_keep]\n",
    "\n",
    "# Step 6: Display the processed DataFrame\n",
    "print(df_filtered.head())\n",
    "\n",
    "# The DataFrame is now stored in df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95a85eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nupur\\AppData\\Local\\Temp\\ipykernel_12432\\999367157.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[col] = pd.to_numeric(df_filtered[col], errors='coerce')\n",
      "C:\\Users\\nupur\\AppData\\Local\\Temp\\ipykernel_12432\\999367157.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered.dropna(inplace=True)  # Drop any rows that may have become NA after conversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4.362572241699714\n",
      "Processed DataFrame:\n",
      "      PX_OPEN  PX_LAST  PX_HIGH  PX_LOW    PX_VOLUME\n",
      "2513   105.15   100.10   110.00   98.30  260752116.0\n",
      "2514    99.40   103.15   103.90   98.00   53192960.0\n",
      "2515   104.00   103.10   104.60  101.55   21271379.0\n",
      "2516   101.20   102.75   103.15  101.00   16597696.0\n",
      "2517   103.25   105.10   107.50  102.60   28063110.0\n",
      "Predictions vs Actual:\n",
      "      Actual  Predicted\n",
      "4914   49.85  50.818412\n",
      "3311   42.70  42.494289\n",
      "4895   23.50  22.114925\n",
      "4579   32.05  31.780880\n",
      "3512   44.45  44.344647\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 1: Connect to the SQLite database\n",
    "conn = sqlite3.connect('Dataset.db')\n",
    "\n",
    "# Step 2: Load data from the SQLite database\n",
    "query = \"SELECT * FROM Power\"  # Replace 'Power' with your actual table name\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Step 3: Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Step 4: Remove rows containing NA values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 5: Keep only the specified columns\n",
    "columns_to_keep = ['PX_OPEN', 'PX_LAST', 'PX_HIGH', 'PX_LOW', 'PX_VOLUME']\n",
    "df_filtered = df[columns_to_keep]\n",
    "\n",
    "# Step 6: Convert relevant columns to numeric\n",
    "for col in columns_to_keep:\n",
    "    df_filtered[col] = pd.to_numeric(df_filtered[col], errors='coerce')\n",
    "\n",
    "# Step 7: Check for any remaining NA values after conversion\n",
    "df_filtered.dropna(inplace=True)  # Drop any rows that may have become NA after conversion\n",
    "\n",
    "# Step 8: Prepare the data for training\n",
    "X = df_filtered[['PX_OPEN', 'PX_HIGH', 'PX_LOW', 'PX_VOLUME']]  # Features\n",
    "y = df_filtered['PX_LAST']  # Target variable\n",
    "\n",
    "# Step 9: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 10: Train a Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 11: Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 12: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Step 13: Create an output directory to save results\n",
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 14: Save the predictions to a CSV file\n",
    "results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "results_df.to_csv(os.path.join(output_dir, 'predictions.csv'), index=False)\n",
    "\n",
    "# Step 15: Display the first few rows of the processed DataFrame and predictions\n",
    "print(\"Processed DataFrame:\")\n",
    "print(df_filtered.head())\n",
    "print(\"Predictions vs Actual:\")\n",
    "print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c6bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
